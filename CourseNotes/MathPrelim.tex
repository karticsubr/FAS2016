\chapterimage{ch2.pdf} % Chapter heading image
\chapter{Mathematical preliminaries}
% We review relevant mathematical concepts and introduce the notation adopted in this course.

% TODO:
% Add figure showing relationship between FT, DFT, DTFT, etc.
% Look at Bartlett 55
% Need to clean up usage of periodogram and power spectrum
% cite http://www.dcs.warwick.ac.uk/~feng/teaching/PowerSpectrum.pdf

\section{Random variables, expectation and variance}
For the purpose of this course, it will suffice to think of random variates simply as variables whose value is subject to chance. 
 For formal definitions of random variables and functions of random variables, we refer interested readers to introductory textbooks on probability or Monte Carlo simulation~\cite{robertc10}.
 We will restrict ourselves to continuous random variables, denoted using bold capital letters (\RV X, \yii,~etc.), that assume values in some domain \Dom. 
 The relative likelihood for continuous random variables to take on a given value is specified using a (normalized) probability density function (pdf) $\sdsym: \Dom \rightarrow \R$. We denote that \RV X\ is distributed according to \sdn\ by $\RV X \sim \sdn$. The probability that the random variable lies in a subdomain $\Domi{i} \subset \Dom$ is obtained as 
% \begin{equation}
  $\Prob(\RV X \in \Domi{i}) = \DefIntT{\Domi{i}}{}{\sdn}{x}.$
% \end{equation}

A function $\phi:\Dom\rightarrow\R$ evaluated at a location specified by a random variable, $\phi(\RV X)$, is also a random variable.
In this course, we are primarily concerned by the first two moments of random variables (specifically functions of random variables). The first moment, or ``average value'' of a random variable is captured by the mathematical concept of the \textit{expected value} of the random variable. We denote the expected value of \RV X\ as 
$ \Expf{\phi(\RV X)}{\sdsym}   \equiv \DefIntT{\Dom}{}{\phi(x)\;\sdsym(x)}{x}$. 

If the pdf has an expected value, the variance of the random variable is its second central moment which we denote
$\Varf{\phi(\RV X)}{\sdsym} \equiv \Expf{\phi^2(\RV X)}{\sdsym} - \Expf{\phi(\RV X)}{\sdsym}^2$. 
When \RV {X}\ is distributed uniformly within the domain (i.e.~\sdn\ is a constant), we drop the subscript and write the expectation as \Exp{\phi(\RV X)} and the variance as \Var{\phi(\RV X)}.


\section{Estimators}
Consider Monte Carlo estimation of the multidimensional integral $ I = \DefIntT{\Dom}{}{\ifn}{x}, \; \x\in\Dom $.A simple \emph{primary} MC estimator for $I$ is
$\estim{1}\equiv\ifsym(\RV X) , \;\RV X \in \Dom$. When \RV X\ is distributed
uniformly, the estimator is unbiased. That is, its expected value is the integral:$\Exp{\estim{1}}=\Ival$. The function \ifsym(\RV X)\ is itself a random variable with an arbitrary distribution and, typically, a large variance. A more practical MC estimator is obtained by averaging a fixed number of (say \N) primary estimates:  $\estim{\N} = \sum \ifni/\N, \; i=1,2,...,\N$. Such \emph{secondary estimators}  are known to be unbiased and approach Gaussian distributions as \N\ is increased, if the primary estimator has finite variance. 


\section{The continuous Fourier transform}

The process of solving differential equations is simplified by projecting relevant functions onto a basis formed by eigenfunctions of the differential operator. The class of functions $x \rightarrow \MyExp{\imag \fvn x}$ forms a natural choice since $\frac {\mathrm{d}}{\mathrm{d}x} \MyExp{\imag \fvn x} = \imag \fvn . \MyExp{\imag \fvn x}$. The projection of a given function, $\ifsym: \Re \rightarrow \C$, results in a set of coefficients --- one coefficient corresponding to every function in the basis (so, per choice of \fvn). If the basis is chosen so that it contains continuous (real) values of \fv, the corresponding coefficients may be viewed as a function of \fv. $\IFsym: \Re\rightarrow\C$. Under certain conditions
\footnote{
We refer readers to textbooks~\cite{bracewell00fourier,kammlerFourier} on the subject for a mathematically rigorous treatment.
}
 the relationship between \ifn\ 
and its coefficients of projection \IFn\ can be written as 
\begin{eqnarray}
 \label{eq:FT} \IFn & = & \DefInt{-\infty}{\infty}{\ifn \; \MyExp{- \imag 2\pi \fv x}}{x}  \\
 \label{eq:IFT} \ifn & = & \DefInt{-\infty}{\infty}{\IFn \; \MyExp{ \imag 2\pi \fv x}}{\fv}.
\end{eqnarray}
The integral transforms in eq.~\ref{eq:FT} and eq.~\ref{eq:IFT} are known as the \textit{Fourier transform} and the \textit{inverse Fourier transform} respectively. Due to Euler's formula in complex analysis,  $\MyExp{\imag 2\pi \fv x} = \cos({2\pi\fvn x}) + \imag \sin({2\pi\fvn x})$, the projection of functions onto complex exponentials is equivalent to a projection onto sinusoids of frequencies given by $2\pi\fv$. For this reason, \fv\ is referred to as the \textit{frequency variable} and \IFn\ is said to be a representation of \ifn\ in the \textit{frequency domain}. We denote the Fourier transform pair using \ifn \Fdual \IFn. In summary, the continuous Fourier transform decomposes a function on the infinite (real) domain into a continuous sum (integral) of sinsuoids of continuous and unbounded set of frequencies. The set of continuous complex coefficients arising from this decomposition is called the \textit{Fourier spectrum} of the function. 

\section{Fourier series} \label{sec:PFT}
The projection of periodic functions onto complex exponentials leads to a special case. Consider a general periodic function \ipn T\ obtained by convolving a function \ifn\ with a Dirac comb \shaf{T} with period $T$. Since $\ipn T = \ifn  \conv \shaf T$, the Fourier transformed periodic function is a product $\IPn T = \IFn \SHAF T$ (due to the convolution theorem). Since \SHAF T\ is also a Dirac comb (with a period of $1/T$ in the frequency domain), the Fourier spectrum of \ipn T\ is only non-zero at the set of locations where \SHAF T\ is non-zero. Therefore, the Fourier spectrum of a continuous periodic function is non-zero at discrete (regular) frequencies.

The inverse Fourier transform of the periodic function can then be written as a summation:
\begin{eqnarray}
  \ipn T & = & \sum\limits_{n\in\Z}^{} c_n \; \MyExp{\imag 2\pi n x / T}.
\end{eqnarray}
The $c_n$, known as the \textit{Fourier series coefficients}, are obtained as 
\begin{eqnarray}
\label{eq:cn}  c_n & = & \frac 1 T \; \DefInt{x_0}{x_0+T}{\ifn \; \MyExp{- \imag 2\pi n x / T}}{x} \;\; = \;\; \frac {\IFsym (n/T)} T
\end{eqnarray}
where $x_0$ is any choice for $x$ so that \ifn\ is integrable within $[x_0, x_0+T]$. Typically the complex exponential in eq.~\ref{eq:cn} is expanded in terms of $\sin{}$ and $\cos{}$, leading to two sets of coefficients. This crucial observation, that even periodic functions with discontinuities may be expressed as the sum of an infinite set of harmonics was made by Joseph Fourier in his study of heat. It was subsequently rigorously proved by Fourier's prot\'eg\'e, Peter Dirichlet, who also outlined requirements for convergence of the Fourier series. Dirichlet's results formed the theoretical foundation which led to the derivation of the continuous Fourier transform. 


\section{Discrete time Fourier transform (DTFT)}
Recall that the Fourier transform of a periodic function results in a discrete spectrum. Conversely, the Fourier transform of a uniformly sampled continuous function results in a periodic Fourier spectrum. Consider the a discrete function obtained by sampling by a Dirac comb.~i.~e.~ the function is $\ifn . \shaf s$, where $s$ is the sampling period. The resulting Fourier spectrum (of the discrete function) will be the convolved spectrum, $\IFn \conv \SHAF s$, which is periodic (with a period of $1/s$).


\section{The Discrete Fourier transform (DFT)}
In practice, Fourier analysis is often performed on sampled functions that span a finite domain. In most cases, the discrete function is also assumed to be replaced by its periodic version with a period equal to the width of the domain. Combining the analysis for Fourier series and DTFT, we can write the input function as $\ipn{sT} = (\ifn . \shaf s) \conv \shaf T$. The Fourier transform of this discrete-time periodic function is $(\IFn \conv \SHAF s) . \SHAF T$ which is periodic (with a period of $1/s$) and discrete due to the frequency-domain sampling by \SHAF T.

Given data $f_n, \; n=0,1,...,N-1$, the $k^{th}$ ($k\in\Z$) coefficients of the DFT and IDFT are 
\begin{eqnarray}
 \label{eq:DFT}c_k &=& \quad \sum\limits_{n=0}^{N-1} f_n \; \left( \cos{(-2\pi k n/N)} + \imag \sin{(-2\pi k n/N)} \right) \;\; \mathrm{and}\\
 \label{eq:IDFT} f_n &=& \frac 1 N \sum\limits_{k=0}^{N-1} c_k \; \left( \cos{(-2\pi k n/N)} + \imag \sin{(-2\pi k n/N)} \right)
\end{eqnarray}
respectively. Due to periodicity, the domain of k is typically $[0,N-1]$. The DFT is a linear, invertible transformation $\mathcal{F}_{DFT}: \C \rightarrow \C$ that exhibits most properties of the continuous Fourier transform such as completeness and orthogonality. Also, important theorems such as the convolution theorem, shift theorem, Plancherel's and Parseval's theorems also have their equivalents in the context of the DFT. Na\"ive calculation of the $N$ coefficients $c_k$ requires $O(N^2)$ computation since each coefficient involves a summation over $N$ terms. Fast Fourier Transform (FFT) algorithms solve this with $O(N \log{N})$ computation.

\section{Power spectral density (PSD) and the periodogram}
The energy of a signal \ifn\ is generally considered to be the integral of the square of the signal (over its entire domain). Due to Parseval's theorem, this may also be expressed as the integral, over all frequencies, of the square (for real signals) of the Fourier transform of the signal. The integrand in this expression of power, $|\IFn|^2$, is referred to as the \textit{power spectrum} of \ifn. 

Now consider a wide-sense stationary random processes, from which a single observation is made. The data from this observation may be subjected to Fourier analysis and the PSD of the observed data --- called a \textit{periodogram} --- may be considered as an estimate of the PSD of the generating process. Although the periodogram is not a good estimate~\cite[Sec.14.2.2]{DimitrisIngle}, it is convenient and hence popular~\cite{journals/cgf/LagaeD08}. 

We denote the PSD of $f(x)$ as $\PowerSpec{f}(\fv)$, which represents the squared amplitude of the Fourier coefficients at the frequency \fv. While we use the same notation for stochastic signals, the interpretation is slightly different. $\PowerSpec{\sfsym}(\fv) \equiv |\SF|^2$ denotes the power spectrum of a single instance of a sampling function. This is not the PSD of the stochastic process, rather it is an estimate of the periodogram $\Exp{|\SF|^2}$. The PSD of the stochastic process is $|\Exp{\SF}|^2$.


\section{Sampling functions in the canonical domain and their spectra} \label{sec:sfn}
We express sampling functions as the sum of weighted unit impulses, or Dirac Delta functions:
\begin{eqnarray}
    S(x) &=& \frac { \sum\limits_{i=1}^{\N} w_i \; \updelta(x-x_i) } {\sum\limits_{i=1}^{\N} w_i }.
\end{eqnarray}
where $x_i \in [0,1]$ are the sampled locations. Sampling of a continuous signal \ifn\ can then be seen as multiplication by the sampling function, and the resulting sampled function is $ \ifn \; S(x)$. If the samples are equally weighted, $w_i = 1/\N$. If the locations $x_i$ are random variables (chosen stochastically), then the resulting sampling function is also a random variable defined across instances of the sampled $x_i$. We are mostly interested with this type of sampling functions, which we will denote using boldface, \sfn, to be consistent with our notation for random variables. We denote the Fourier transformed sampling function using \SF. Since this is also a random variable defined over specific instances of \N\ sample locations, we may define its expectation $\Exp{\SF}$ and variance \Var{\SF}. The $k$-sample periodogram of the sampling function is obtained by averaging $k$ estimates of $|\SF|^2$. The PSD of the sampling process is $|\Exp{\SF}|^2$. Clearly, in this case, the periodogram is not an accurate estimator for the PSD, since averaging is performed without phase information. 



\begin{table}[hbpt]%
\caption{\label{tab:notation}%
A summary of notation used in this course.\TBC}%
\setlength{\extrarowheight}{0.3cm}
\begin{tabular}{rl}%
    \toprule
    Symbol & Definition\\
    \midrule
    \ifn 	&  integrand \\
    $\w(x)$	&  sample weights\\
    \sdn 	&  sampling distribution (pdf) \\
    \estim{\N} 	&  estimate of $I$  using \N\ samples \\
%     \estim{is,\N} 	&  \N-sample importance sampling estimator\\
%     \estim{is,1} 	&  primary (1-sample) imp. samp. estimator\\
    \RV X, \RV Y, \xii, \yii & random variates \\
    $\PowerSpec{S}(\fv)$ &
	  Power spectrum of a single instance of \sfn\ (periodogram)\\ %: $\PowerSpec{S}(\fv) = $
    $\sfn \Fdual \SF $&
          Fourier transform pair: integrand and its transform\\
    $\sfn \Fdual \SF $&
          Fourier transform pair: sampling function and its transform\\
    $\Exp{\phi(\RV X)}$ 		&
      Expectation: $\DefIntT{\Dom}{}{x\;\phi(x)}{x}$\\[-2pt]
    $\Var{\phi(\RV X)} $ 						&
      Variance : $\DefIntT{\Dom}{}{x\;\phi^2(x)}{x} -
      \DefIntT{\Dom}{}{x\;\phi(x)}{x}$\\
    $\Expf{\phi(\RV X)}{\sdsym} $ &
      Expectation with $ \RV X \sim \sdn$:
$\DefIntT{\Dom}{}{\phi(x)\;\sdsym(x)}{x}$\\[-2pt]
% $\Exp{\phi^2(\RV X)} - \Exp{\phi(\RV X)}^2$\\
    $\Varf{\phi(\RV X)}{\sdsym} $					&
Variance with $ \RV X \sim \sdn$:
  $\DefIntT{\Dom}{}{\phi^2(x)\sdn}{x} -
      (\DefIntT{\Dom}{}{\phi(x)\sdn}{x})^2$\\
\bottomrule
\end{tabular}
\end{table}
