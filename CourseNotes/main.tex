%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The Legrand Orange Book
% LaTeX Template
% Version 2.1 (14/11/15)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Mathias Legrand (legrand.mathias@gmail.com) with modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%by a multiplication 
% Compiling this template:
% This template uses biber for its bibliography and makeindex for its index.
% When you first open the template, compile it from the command line with the 
% commands below to make sure your LaTeX distribution is configured correctly:
%
% 1) pdflatex main
% 2) makeindex main.idx -s StyleInd.ist
% 3) biber main
% 4) pdflatex main x 2
%
% After this, when you wish to update the bibliography/index use the appropriate
% command above and make sure to compile with pdflatex several times 
% afterwards to propagate your changes to the document.
%
% This template also uses a number of packages which may need to be
% updated to the newest versions for the template to compile. It is strongly
% recommended you update your LaTeX distribution if you have any
% compilation errors.
%
% Important note:
% Chapter heading images should have a 2:1 width:height ratio,
% e.g. 920px width and 460px height.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[11pt,fleqn]{book} % Default font size and left-justified equations
\usepackage{rotating}
\usepackage{grffile}
\usepackage{xcolor,colortbl}
\usepackage{multirow}
\usepackage[export]{adjustbox}
\usepackage{mathtools}
\usepackage[final]{pdfpages}
\usepackage{microtype}

% \newcommand{\TBC} {\vspace {1em} \noindent [TO BE COMPLETED IN THE FINAL VERSION.] \vspace {1em}}
% \newcommand{\TBC} {\noindent [TO BE COMPLETED IN THE FINAL VERSION.] }
\newcommand{\TBC} {}

%----------------------------------------------------------------------------------------

\input{structure} % Insert the commands.tex file which contains the majority of the structure behind the template
\input{macros-gurprit}
\input{macros}
\begin{document}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begingroup
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
\coordinate [below=12cm] (midpoint) at (current page.north);
\node at (current page.north west)
{\begin{tikzpicture}[remember picture,overlay]
\node[anchor=north west,inner sep=0pt] at (0,0) {\includegraphics[width=\paperwidth]{background}}; % Background image
\draw[anchor=north] (midpoint) node [fill=ocre!30!white,fill opacity=0.6,text opacity=1,inner sep=1cm]
  {\Huge\centering\bfseries\sffamily\parbox[c][][t]{\paperwidth}
%    {\centering Fourier Analysis of Sampling Patterns \\ for Rendering: Theory and Practice \\[15pt] % Book title 
   {\centering Fourier Analysis of Numerical Integration in \\ Monte Carlo Rendering: Theory and Practice \\[15pt] % Book title
   {\Large Understanding estimation error in Monte Carlo Image Synthesis}\\[20pt] % Subtitle
   {\huge Kartic Subr, Wojciech Jarosz and Gurprit Singh}}}; % Author name
\end{tikzpicture}};
\end{tikzpicture}
\vfill
\endgroup
\newpage
% %----------------------------------------------------------------------------------------
% %	COPYRIGHT PAGE
% %----------------------------------------------------------------------------------------
% 
% \newpage
% ~\vfill
% \thispagestyle{empty}
% 
% \noindent Copyright \copyright\ 2016 --------\\ % Copyright notice
% 
% \noindent \textsc{Published by --------}\\ % Publisher
% 
% % \noindent \textsc{Best-Course.com}\\ % URL
% 
% \noindent Copyright information to be inserted here.
% 
% % \noindent Licensed under the Creative Commons Attribution-NonCommercial 3.0 Unported License (the ``License''). You may not use this file except in compliance with the License. You may obtain a copy of the License at \url{http://creativecommons.org/licenses/by-nc/3.0}. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \textsc{``as is'' basis, without warranties or conditions of any kind}, either express or implied. See the License for the specific language governing permissions and limitations under the License.\\ % License information
% 
% \noindent \textit{July 2016} % Printing/edition date
% 
% \newpage

%----------------------------------------------------------------------------------------
%	ABSTRACT PAGE
%----------------------------------------------------------------------------------------


% \pagestyle{plain}
\pagenumbering{roman}

{

% % \cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Abstract}
\section*{Abstract}
Since being introduced to graphics in the 1980s, Monte Carlo sampling and integration has become the cornerstone of most modern rendering algorithms. Originally introduced to combat the effect of aliasing \KARTIC{Is that true? Wasn't it cook with distributed ray tracing?} when estimating pixels values, Monte Carlo has since become a more general tool for solving complex, multi-dimensional integration problems in rendering. In this context, MC integration involves sampling a function at various stochastically placed points to approximate an integral, e.g. the radiance through a pixel integrated across the multi-dimensional space of possible light transport paths. Unfortunately, this estimation is error-prone, and the visual manifestation of this error depends critically on the properties of the integrand, placement of the stochastic sample points used, and the type of problem (integration vs.\ reconstruction) that is being solved with these samples.

We describe how errors present in rendered images may be analyzed as a function of the spectral (Fourier domain) statistics of the underlying sampling patterns fed to the renderer.  Fourier analysis, along with the Nyquist theorem, has long been used in graphics to motivate more intelligent sampling strategies which try to minimize errors due to noise and aliasing in the \textit{pixel reconstruction problem}. Only more recently, however, has the community started to apply these same Fourier tools to analyze error in the Monte Carlo \textit{integration problem}. Loosely speaking, in the context of rendering a 2D image, these two problems are concerned with errors introduced across pixels (reconstruction) vs.\ the errors introduced within any individual pixel (integration). In this course, we focus on the latter, and survey the recent developments and insights that Fourier analyses have provided about the magnitude and convergence rate of Monte Carlo integration error. We provide a historical perspective of Monte Carlo in graphics, review the necessary mathematical background, summarize the most recent developments, discuss the practical implications of these analyzes on the design of Monte Carlo rendering algorithms, and identify important remaining research problems that can propel the field forward.


\addcontentsline{toc}{chapter}{Relationship to courses previously presented at SIGGRAPH}
\section*{Relationship to courses previously presented at SIGGRAPH}
Physically based rendering is a broad area that subsumes many active sub-areas of research.  
Rendering courses, which have been previously presented at SIGGRAPH, have focused on aspects of physically-based shading computation, the benefits of specific algorithms for rendering, summarization of state-of-the-art algorithms for light transport simulation, etc. There have also been courses on the fundamentals of Monte Carlo and quasi Monte Carlo sampling in the context of rendering. The focus of this course is complementary to previous rendering courses. We do not expect (or require) the audience to have attended any of the aforementioned courses.

Fourier analysis is also extensively used in the design of computational displays and cameras. Our focus on numerical integration is fundamentally different than those applications, where integration happens physically (implicitly) by propagation of real light energy  (e.g.~blurring of images due to a wide aperture). There have been conspicuously few courses on the application of Fourier analysis at SIGGRAPH and we hope that the sections containing generalized treatment of the problem will provide theoretical insights into applications other than rendering. 
}

\section*{Intended audience, prerequisites and level of difficulty}
This course is designed for graduate students and professionals seeking to understand Monte Carlo rendering, Fourier analysis and/or errors due to sampling. We expect attendees to possess undergraduate-level proficiency in probability theory and calculus. We will begin by reviewing much of the intuition required for the appreciation of an intricate combination of Monte Carlo rendering, Fourier analysis, sampling theory and probability, which is the focus of this course.
\clearpage


%----------------------------------------------------------------------------------------
%	LECTURERS PAGE
%----------------------------------------------------------------------------------------

\phantomsection
\addcontentsline{toc}{chapter}{About the lecturers}
\section*{About the lecturers}

\noindent\textbf{Prof.\ Kartic Subr} \hfill
\href{<k.subr@hw.ac.uk>}{\texttt{<k.subr@hw.ac.uk>}} \\
Heriot Watt University\hfill
\url{http://home.eps.hw.ac.uk/~ks400/}\\

Kartic Subr is an Associate Professor of Engineering at Heriot Watt University and a Royal Society University Research Fellow. He has enjoyed a variety of positions in companies such as Hewlett Packard, Rhythm and Hues Studios, NVIDIA Corporation and Disney Research as well as academic post-doctoral research positions at INRIA-Grenoble and University College London. He obtained his Ph.D. (2008) and M.S. (2005) in Visual Computing from UC Irvine. 

Kartic's research explores the benefits of stochastic and adaptive sampling when applied to a variety of classes of problems encountered in computer graphics such as numerical integration, signal reconstruction, edge-preserving smoothing of images, assisted segmentation of images and image completion. More recently, he has focused on error analyses of popular stochastic sampling strategies for numerical integration in the context of simulating light transport for rendering.

\medskip
\textcolor[RGB]{220,220,220}{\rule{.9\columnwidth}{0.5pt}}
\medskip

\noindent\textbf{Prof.\ Wojciech Jarosz} \hfill
\href{mailto:wojciech.k.jarosz@dartmouth.edu}{\texttt{<wojciech.k.jarosz@dartmouth.edu>}} \\
Dartmouth College\hfill
\url{http://www.cs.dartmouth.edu/~wjarosz}\\

Wojciech Jarosz is an Assistant Professor of Computer Science at Dartmouth College and co-founder of Dartmouth's Visual Computing Lab. Before joining Dartmouth in 2015, Wojciech was a Senior Research Scientist heading the rendering group at Disney Research Zürich, and an adjunct lecturer at ETH Zürich. He obtained his Ph.D. (2008) and M.S. (2005) in computer graphics from UC San Diego, and a B.S. (2003) in computer science from the University of Illinois at Urbana–Champaign.

Prof.\ Jarosz’s research covers many areas of computer graphics and rendering, with a particular emphasis on \emph{light}: how to capture it, how to more efficiently simulate its interaction and propagation within scenes, how to author or edit appearance, and how to create physical objects with prescribed light interactions. Much of his work has involved developing more sophisticated sampling techniques in the context of Monte Carlo integration and the resulting methods have been incorporated into production rendering systems for the making of feature films, including Disney's \textit{Tangled} and \textit{Big Hero 6}. In 2013, Prof.\ Jarosz received the Eurographics Young Researcher Award.

% \bigskip
\medskip
\textcolor[RGB]{220,220,220}{\rule{.9\columnwidth}{0.5pt}}
\medskip

\noindent\textbf{Dr.\ Gurprit Singh} \hfill
{\texttt{<gurprit.singh@dartmouth.edu>}} \href{mailto:gurprit.singh@dartmouth.edu}\\
Dartmouth College\hfill
\url{http://www.cs.dartmouth.edu/~gsingh}\\

Gurprit Singh is a post-doctoral researcher at Dartmouth College, working with Prof. Wojciech Jarosz in the Dartmouth's Visual Computing Lab. Before joining Dartmouth in 2015, Gurprit was a PhD candiate 
at the Univerisit\'{e} de Lyon 1, France, where he successfully obtained his doctorate (2015) under the 
supervision of Prof. Victor Ostromoukhov. Gurprit did his Masters (2012) from INP Grenoble, France and 
obtained his bachelor's thesis (2010) from IIT Delhi, India, under the supervision of Prof. Prem Kalra and Prof. Niloy J. Mitra.

Gurprit's research is mainly focused on understanding point sample distributions from the Fourier perspective. During his thesis work, Gurprit developed a theoretical model in the spherical domain that can characterise various sampling methods (jitter, Poisson Disk, Blue noise) based on their frequency content. This theoretical framework can be used to predict the exact variance for Monte Carlo integration. Later on, this framework is also used to analytically derive the variance convergence rates 
of various well known sampling patterns (jitter, Poisson Disk) that are extensively used in production rendering.
% \cleardoublepage



%----------------------------------------------------------------------------------------
%	COURSE SYLLABUS
%----------------------------------------------------------------------------------------

% \phantomsection
\addcontentsline{toc}{chapter}{Course Syllabus}
\chapterimage{syllabus.pdf} % Table of contents heading image
\section*{Course Syllabus}
%
\input{table-schedule}
%
%\GURPRIT{Needs to be updated}
%Photon mapping techniques have progressed significantly in the last few years. The first part will include a brief introduction to light transport and the core ideas behind photon mapping. The remainder of the time will focus on recent advances in photon density estimation for surfaces.
%
%The second part will start with a brief introduction to participating media, followed by in-depth presentations on recent developments in volumetric photon mapping. The last portion of the course will cover the uses of photon density estimation in industry.
%
%These topics are shown below:
%
%A detailed breakdown in provided in Figure~\ref{fig:half-day}. Each slot is 5~min.
%
%\begin{figure}[b!]
%\begin{center}
%\end{center}
%\caption{%
%\label{fig:half-day}%
%Session timing breakdown for the course.
%}
%\end{figure}



%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

\chapterimage{contents.pdf} % Table of contents heading image

% \pagestyle{empty} % No headers

\phantomsection
\addcontentsline{toc}{chapter}{Table of Contents}
\tableofcontents % Print the table of contents itself

% \cleardoublepage % Forces the first chapter to start on an odd page so it's on the right

% \pagestyle{fancy} % Print headers again
\newpage
%----------------------------------------------------------------------------------------
%	CHAPTER 1
%----------------------------------------------------------------------------------------


\chapterimage{ch1.pdf} % Chapter heading image
\pagenumbering{arabic}

\chapter{Introduction}
% Evolution has honed the ability of our visual systems to interpret visual snapshots, or images of our environments, that are sensed by our eyes.
Persuading our sophisticated visual systems of the ``realism'' of computer graphics imagery is a challenging task. 
This problem is exacerbated by the presence of fantastic objects and/or characters embedded in the virtual scene, thereby loosening the notion of realism in the resulting depiction. 
Realistic depiction is typically achieved by an artful combination of design considerations such as the degree of detail in geometrical modelling of objects in the environment, application of plausible models for dynamics, tactful animation of objects and characters, the choice of display technology used to present the visuals to the observer, etc. 

While the totality of the above decisions impacts the illusion of reality, the focus of this course is on a vital component in the pipeline --- \textit{the synthesis of photorealistic images} given the various models that describe the environment. The state of the art in the generation of plausible pictures -- which we will henceforth refer to as \textit{rendering} --  involves simulation of the physics of light. 
The simulation of light transport is typically tedious and error-prone.
Error-free rendering is a computationally expensive process that needs to be performed repetitively, on every frame, to generate high (production) quality video sequences. 


\section{Rendering}\index{Rendering}
We refer to rendering as the process of producing a picture which represents a particular view of an environment, captured using a virtual camera, at a given time. The input to the rendering system, or renderer, is a detailed representation of all the objects in the scene. This representation is typically provided in a \textit{scene description file} that specifies the objects in the scene, their shapes, their material properties (rough, smooth, shiny, diffuse, translucent, etc.), their motion parameters and whether these objects emit light energy. In addition to the environment, the renderer is also provided with a description of the virtual camera used to capture the rendered view including its position, orientation, optical characteristics (any lenses, aperture, etc.), shutter parameters (exposure time)  and parameters of the virtual sensor (aspect ratio, optical sensitivity, etc.) embedded within it. 

\TBC 
\begin{figure}
 \centering
 {
  \includegraphics[width=\linewidth]{RenderHL.pdf}
 }
 \caption{\emph{The renderer implicitly maps samples from some high-dimensional space to light paths in a specified scene. The contributions of radiant light energy along these paths are recorded on the image plane as virtual measurements. Three light paths (a,b,c) are shown along with their canonical samples in the (hyper-) cube. The next two steps of rendering are integration along the path-dimension (step 1) followed by reconstruction on a regular grid (step 2). In this course, we focus on the impact of Fourier spectral properties of the canonical samples on the errors due to numerical integration.}}
 \label{fig:renderhl}
\end{figure}


\section{The interplay between reconstruction and integration}\index{Reconstruction and integration}
Images are typically represented and displayed on a regular grid. Rendering algorithms, however, may estimate virtual measurements at arbitrarily points on the sensor. We refer to the process of resampling the measurements \Ij{}\ on a regular grid as \textit{2D reconstruction}. Some rendering algorithms~\cite{Hachisuka:2008,Egan:2009,Soler:2009,Cov5D} partition the space of paths ($\Omega$ in equation~\ref{eq:measeq}) explicitly into dimensions that are dependent on the camera, such as exposure time and aperture, and those that depend on the scene. Since the sensor is two-dimensional, exposure time is one-dimensional and the aperture is two-dimensional, they estimate virtual measurements in a five-dimensional, camera-dependent space. This involves a reconstruction in 5D followed by a further projection (integration) down to the two-dimensional plane of the sensor. Such a partitioning is exploited for efficient rendering of camera-dependent blur effects resulting from relative motion between the camera and the scene or from defocus due to a finite (rather than infinitesemal) aperture.

\TBC

\section{The role of sampling}
There are two distinctly different applications of sampling in the rendering process (see figure~\ref{fig:renderhl}). One of them is for reconstruction, either in 2D or in 5D (as described above), and the other is for estimation of integration, which lies at the core of light transport (equation~\ref{eq:measeq}). Most rendering algorithms focus on the latter and perform reconstruction on a two-dimensional regular grid of pixels. Path-tracing, a popular rendering algorithm, proceeds by estimating the virtual image measurement at each pixel by averaging the values of the measurement contributions of a chosen set of paths through each pixel. Each path in the chosen set is obtained via a mapping from a sampling pattern (of appropriate dimensionality) to a set of rays in the scene. These sampling patterns may be stochastic (Monte Carlo sampling) or deterministic patterns that satisfy desirable criteria (Quasi-Monte Carlo). The statistics of the estimates of the virtual measurements yielded by these two broad classes of algorithms are strikingly different, as is the mathematical machinery used for their analyses.  

\TBC

\section{Sources and manifestation of error}\index{Error in rendering}
Error in the context of estimating global illumination~\cite{arvo1994framework} arises from three sources: inaccuracies in the  input data or simulation, errors due to discretization and computational errors such as loss of precision. In this course, we focus on the first two sources. The rendered image is represented on a discrete grid, while the distribution of the underlying light energy across the sensor is a continuous function. A major source of error stems from this discretization. The underlying continuous function is a high-dimensional integral at each point on the sensor (equation~\ref{eq:measeq}). Any sampling-based approach used to estimate this function results in an approximation whose measured deviation from the true function is known referred to as \textit{approximation error}. Typically this error manifests in rendered images as structured artifacts or noise. 
% Various metrics, such as L1 norms, L2 norms, PSNR, etc., are used to quantify approximation error.

\TBC 

%----------------------------------------------------------------------------------------
%	CHAPTER 2
%----------------------------------------------------------------------------------------
\input{MathPrelim.tex}

%----------------------------------------------------------------------------------------
%	CHAPTER 3
%----------------------------------------------------------------------------------------
\chapterimage{ch3.pdf} % Chapter heading image
\chapter{Sampling for reconstruction}
The reconstruction problem entails approximating a function $\ifsym:\Dom\rightarrow\R$ as $\appr{\ifsym}(x_i), \; x_i\in\Dom, \;i=1,2,3,...,M$ given $\ifsym(y_i), \; y_i \in \Dom, \;j=1,2,3,...,\N$, where the sets of points $\{x_i\}$ and $\{y_j\}$ are mutually exclusive. If $\appr{\ifsym}(y_j) = \ifsym(y_j),\;\forall j$, the process is referred to as interpolation. This problem has received much attention in the signal processing literature~\cite[chapters~5~\&~6]{engineering2008handbook} as well as in the rendering community~\cite{Wold85,Cook:1986:SSC,mitchell87a,journals/cga/Blinn89a}. Such analyses have yielded effective reconstruction filters that are used in modern renderers, especially in low-dimensional domains such as the image plane.

Reconstruction is akin to nonlinear regression~\cite{GVK394929411}, a popular problem in the statistics (and more recently machine learning) literature, which offers effective solutions in high-dimensional settings when the approximand exhibits stationarity or accurate solutions for smooth and low-dimensional approximands. In rendering, the underlying functions represent a multitude of combinations (products, convolutions and sums) of a variety of non-stationary processes such as spatio-directional lighting, hard shadows, sharp reflections, textured materials, etc. The resulting discontinuities and variations in the radiant light energy (radiance function) across space-angle-time pose immense challenges which state of the art methods in non-linear regression are unable to cope with.

The reconstruction process generally consists of two steps. First, for any point $x$ in the domain, the influence of each of the $\ifsym(y_j)$ samples is specified using some \textit{weighting kernel} on the distance between $x$ and $y_i$ (typically Euclidean). Then, at each of the $x_i$, the approximate value is obtained by summing up the distance-weighted contributions of each of the $\ifsym(y_j)$. Depending on the choice of the kernel and the strategy for determining weights, the resulting methods are known by different names: Kernel Smoothers, Radial Basis Functions, Shepherd's method, etc.  

\section{Aliasing in reconstruction}
The general reconstruction process (above) may be written in terms of the underlying function (or signal). The samples, $\ifsym(y_i), \; y_i \in \Dom$, can be represented as the  product of the true (often unknown) function \ifn\ with a sampling function of the form $S(x) = \sum\limits_{j=1}^{\N} \updelta(x-y_j)$. 
If $K(r)$ represents the kernel weight at a distance of $r$, the continuous approximand and its Fourier dual are
\begin{equation} \label{eq:recons}
 \appr{\ifsym}(x) = \left(\ifn . S(x) \right) \conv K(x) \Fdual 
 \appr{\IFsym}(\fv) = \left(\IFn \conv \FTsym{S}(\fv)\right) . \FTsym{K}(\fv) 
\end{equation}
where \conv\ denotes convolution. That is, the effect of sampling can be seen in the Fourier domain as one of generating spurious replicas of \IFsym\, created due to the convolution with $\FTsym{S}$, which potentially hinders perfect reconstruction.   The kernel, on the other hand, can be viewed as a low-pass filter which suppresses replicated, or aliased, copies of the true function's spectrum at high frequencies. 

If \ifn\ contains frequencies less than $\fv_b$ (is band-limited), if the sampling spectrum does not contain frequencies lower than $2\fv_b$, and if the kernel is a low-pass filter suppressing frequencies above $\fv_b$, the resulting reconstruction will be perfect. If these conditions are not satisfied, the approximation will contain error. The first potential source of error is that \ifn\ is not bandlimited and hence, regardless of the sampling function, the low-pass effect of the kernel results in the loss of high-frequencies. The other main source of error is due to the sampling spectrum containing low frequencies, which results in a \FTsym{\appr{\ifsym}}\ that contains a superposition of \IFn\ with its aliased copies.

\section{Antialiasing}
The error introduced by aliasing either appears as coherent structure or as noise, depending on the choice of sampling spectrum. Sampling on a regular grid corresponds to multiplication with a Dirac-train or Shah function, whose Fourier spectrum remains a Shah function. If the samples are not sufficiently dense in the image plane, the resulting spectrum contains impulses at lower frequencies which cause aliasing in the form of regular structure that has been shown to be particularly unpleasant to our perceptual system. If the function being reconstructed is bandlimited, and in the absence of a sampling budget, the sampling rate may be increased to eliminate aliasing. However, if the function contains arbitrarily high frequencies, a simple strategy is to introduce randomness in the spectrum thereby transforming coherent artifacts into visual noise. This has shown to be less objectionable to our visual systems.

\TBC

\section{Halftoning}
Halftoning is the process of determining a sampling function $S(x)$ so that a kernel density estimate of the sampled function $\appr{\ifsym}(x) = \ifn.S(x)$ provides the best approximation to \ifn. It is a special case of reconstruction where the kernel is implicit and dependent on our visual systems. Halftoning methods strive to identify sampling functions for which the \textit{perceived} sampled function is as close to \ifn\ as possible. This may be achieved by varying the distributions of the samples, by weighting them or using a combination of both. Densely sampled regions of the sampled function are perceived to have darker tone. 

\TBC

\section{Antialiasing as integration}
Antialiasing is commonly viewed as an integration problem. The value to be assigned to a pixel is often estimated by averaging sampled values within a square around the pixel. That is, pixels are viewed as little squares and the values assigned to pixels are obtained by integrating the underlying function within their respective squares. On the contrary, if pixels are viewed as simply being points on the image plane~\cite{smith1995pixel}, this choice of integrating within the pixel appears to be an arbitrary choice. That is, it corresponds to performing reconstruction on the image plane exactly as in equation~\ref{eq:recons}, with $K(x)$ chosen to be a box with a width that is half the inter-pixel distance, followed by resampling of $\appr{\ifsym}(x)$ on a regular grid. Although this view of antialiasing is better than not performing antialiasing, it is well known that the box-filter is not the ideal choice. Modern renderers offer a variety of choices for the pixel reconstruction filter, since its choice is dependent on the rendering algorithm chosen and the corresponding sampling patterns used for estimating light energy arriving at the pixel~\cite[section~7.6]{Pharr:2010:PBR:1854996}.

\TBC
%----------------------------------------------------------------------------------------
%	CHAPTER 4
%----------------------------------------------------------------------------------------
\chapterimage{ch4.pdf} % Chapter heading image
\chapter{Estimating integrals}
It has long been known that the problem of numerical integration lies at the heart of rendering~\cite{Perlin89Course}. The main objective of rendering is to estimate the quantity of radiant light energy impinging on the various pixels (or cells) distributed on a virtual observer's camera (or eye). Current renderers perform this via physically-based simulation of light, following its intricate combinations of reflections within the scene. The virtual measurement \Ij {\xs} at a point \xs\  on the sensor~\cite[ch.8]{Veach:1998:RMC:927297} is
\begin{eqnarray} \label{eq:measeq}
 \Ij {\xs} & = & \DefInt{\Omega}{}{f_\xs(\pth)}{\mu(\pth)}
\end{eqnarray}
where the measurement contribution function, $f_\xs(\pth)$, is the importance-weighted radiance arriving at \xs\ along the path \pth\ and $\mu(\pth)$ is a measure on the space of all paths $\Omega$. The paths \pth\ span space as well as time and may be of arbitrarily high dimensionality. 

The exact nature of the integrand depends on the complexity and structure of the scene being rendered, along with detailed modelling of the constituent materials and shapes.  In general, the integrand is high-dimensional, discontinuous and costly to evaluate. Each evaluation of the integrand requires many rays to be traced within the environment, along with estimation of the radiant light energy along that path formed by linking the rays together. 
The rendering community strives to estimate such integrals with low error as well as low computational cost.


\section{Numerical integration via sampling}
Henceforth in this chapter, we abstract away the details about the integrand and domain of integration (equation~\ref{eq:measeq}) that is encountered in rendering. We study the integration problem in the canonical domain.  
Our goal is to analyze the of quality of numerical integration, within the interval $[0,\Tx]$, of a function $\ifsym: \R \mapsto \R^{+}$. 
% in terms of the Fourier spectrum of the underlying sampling strategy. 
For example, a \textit{primary} Monte Carlo estimator $\estim{1} = \ifsym(\xii)$ for the integral
\begin{equation} \label{eq:integcanonical}
I \equiv \frac 1 \Tx \DefInt{0}{\Tx}{\ifn }{\x}  
\end{equation}
is well known to be unbiased if the  single sample $\xii \in [0,\Tx] $ is drawn from a constant probability distribution function (pdf) within the domain $[0,\Tx]$.~i.e.~$\Exp{\estim{1}} = I$.
The corresponding secondary estimator, obtained by averaging \N\ primary estimates at different \xii\ has the same expected value. However, the averaging scales the variance down by a factor of $\N$.~i.e.~$\Var{\estim{\N}} = \Var{\estim{1}}/\N.$ 

\TBC
\begin{figure}
 \centering
 {
  \includegraphics[width=\linewidth]{IntegRecons.pdf}
 }
 \caption{\emph{This figure highlights the difference in the analysis of sampling rates for error-free reconstruction vs numerical integration of a bandlimited function. Fewer samples (factor of 2 per dimension~\cite{Cov5D}) are sufficient for error-free numerical integration.}}
\end{figure}

\section{Error due to sampling: Convergence rate, consistency, bias and variance} 
Numerical integration methods like Monte Carlo are very powerful in approximating the underlying  multi-dimensional integrand using stochastically generated samples, but the overall process is highly prone to error. The nature of the error introduced depends on an intricate combination of three factors: the distribution of the sample locations; the weighting used to accumulate sampled contributions; and the arrangement (correlation) and structure of samples. The ``error'' of an estimator, which refers to its mean squared error (MSE), typically reduces as the number of samples is increased. The rate at which this reduction occurs is known as the estimator's \textit{convergence rate}. Estimators whose errors vanish in the limit are said to be \textit{consistent}.

The MSE of an estimator is comprised of two different types of error: One that is associated with the accuracy of the estimator, and another which describes the precision of the estimates. The former, which is the deviation of the expected value from the true (or desired) result is known as \textit{bias}. The latter, which quantifies the uncertainty of the estimator is known as \textit{variance}. The MSE is expressed as the sum of the squared bias and the variance.

Consistent estimators, whether biased or unbiased, are desirable for image synthesis. Although objectionable image artifacts may be produced by the use of na\"ively-designed biased estimators (say using regular sampling), in many cases connsistent and biased estimators are used to reduce noise as well as to improve convergence rates~\cite{Keller:2012:AMCCourse}. 

% Bias is defined as the value by which the numerically integrated value deviates from the ground truth value. In the Fourier domain, bias can be represented as an expected value of the error~\cite{Subr:2013:FAS}. Another form of error is variance which is manifested in the form of noise during image synthesis. Variance represents the uncertainty in the primary Monte Carlo estimates. 

%\subsection{Variance (uncertainty of estimates)} 
%Another form of error is variance which is manifested in the form of noise during image synthesis. %Variance represents the uncertainty in the primary Monte Carlo estimates. 

%\GURPRIT{
%Unless I am missing something I don't see why uncertainty refers to Variance. For me uncertainty is a relative concept. For example, 
%the variance of a kernel in the primal and the fourier domain cannot be predicted simultaneously at a given frequency. Reducing variance in one domain would increase variance in the other, which is what 
%uncertainty is all about. }
%\KARTIC{Random variables introduce uncertainty. Variance is one form of the quantification of uncertainty. If you are estimating Variance, then you can talk about uncertainty of the variance estimates. I like to pose the process of quantifying error in the estimates as a form of uncertainty propagation (\url{https://en.wikipedia.org/wiki/Propagation_of_uncertainty}). That is, if we know the uncertainty in the sampling patterns, can we estimate uncertainty in the final estimates?}

\input{manifestationErrorRendering}

\subsection{Homogenization}
The analytical characterization of both, bias as well as variance, across a general set of sampling strategies is challenging. 
% Analysing both bias and variance at the same time in Monte Carlo integration can be challenging. 
 However, focusing on unbiased estimators makes it possible to express error in the form of variance only. To apply this analysis to a wide variety of estimators, it would be beneficial to first be able to transform them into unbiased estimators, albeit by introducing extra variance. 
 One way to mitigate the bias of an estimator is by ensuring that the sampling spectrum only contains energy at the DC~\cite{Subr:2013:FAS}. Inspired by the process of generating unbiased estimators from randomly jittering a regular grid~\cite{}, Pilleboue et al~\cite{Pilleboue:2015:VAM} shifted each realization of the samples by uniformly random amounts. They termed this process \textit{homogenization}. In the point processes literature homogenization is also known as \emph{stationary point processes} for which the average number of points per some unit of extent such as length, area, volume, or time, is constant depending on the underlying mathematical space.

%\subsection{Consistency}
%\TBC

% \subsection{Consistency and Convergence}
% Sampling strategies for which error goes down with the increase in number of samples are considered consistent. Although, reducing variance in Monte Carlo estimators have been given paramount importance in the past, it is also worth while to investigate the convergence rate, which represents the rate at which error goes down with the increase in the number of samples. 

\section{Monte Carlo integration in the Fourier domain}
Monte Carlo sampling can be represented as an inner product of the integrand \ifn\ and a random sampling pattern \sfn\ which is composed of a sum of impulses (delta functions) normalized by the number of impulses (see Sec.~\ref{sec:sfn}). Since inner products are preserved under Fourier Transforms, the na\"ive estimator  can be expressed in the frequency domain as 
\begin{equation}
  \estim{\N} \quad = \quad \DefInt{\Dom}{}{\ifn \sfn}{x} \quad = \quad 
  \DefInt{\Omega}{}{\SF \; \IFsym^*(\fv) }{\fv}, 
\end{equation}
where $\IFsym^*(\fv)$ is the complex conjugate of the Fourier transformed integrand. Since the goal is to represent the spectrum of a stochastic process, which is generally not integrable over the infinite domain, it is important that the integral is restricted to the finite domain \Dom. The definite integral can either be viewed as a multiplication by a box of the appropriate dimensions~\cite{Subr:2013:FAS} or as a portion of a periodic function on a toroidal domain~\cite{Pilleboue:2015:VAM}. The former leads to a 'virtual' integrand which subsumes the convolution by a \sinc (the Fourier transformed box), while the latter leads to a discrete Fourier spectrum (see Sec.~\ref{sec:PFT}). This reformulation of numerical integration provides a fresh perspective on the analysis of error. The derivation of MSE~\cite{FredoTR}, convergence~\cite{Pilleboue:2015:VAM}, bias and variance~\cite{Subr:2013:FAS} in terms of the statistics of the Fourier spectra of the sampling function and the integrand provide valuable insight into the design of adaptive sampling strategies.


%\section{Assessing sampling patterns based on their spectra}
%
\input{assessingSamplingPatterns}
%

\section{Analysis beyond the canonical domain}
In rendering, Monte Carlo integration is not restricted to the Euclidean domain. For example, direct and global illumination methods involve integration over a visible  hemisphere of directions at the surface hit points, and over the sphere of directions in the presence of participating media within the scene. In this section we discuss a variety of such contexts. In particular, we focus on a closed form formulation for variance in the ( hemi-)spherical domain. 

\input{SectionCanonicalDomain/sphericalDomain}

\input{SectionCanonicalDomain/generalDomain}

% \input{SectionCanonicalDomain/gradientDomain}

%----------------------------------------------------------------------------------------
%	CHAPTER 5
%----------------------------------------------------------------------------------------
\chapterimage{ch5.pdf} % Chapter heading image
\chapter{Popular sampling patterns} \label{ch:stateoftheart}
In this chapter, we will describe sampling strategies that are popularly used in modern renderers. For each of these, we provide insight on its error, explain the contexts where its behavior is best and/or worst and also comment on its expected convergence rate using its homogenized periodogram. For simplicity we will primarily discuss these sampling routines in the context of generating locations within a canonical unit square interval in 2D, and briefly explain how each routine generalizes to higher dimensions. Example slides for this topic are included (chapter~\ref{ch:exslides}).

\TBC

\section{Classical}
\subsection{Independent random sampling}
Purely independent random sampling is the most basic form of sampling pattern. When uniformly sampling a 2D square domain, the $x$ and $y$ coordinates of each point are computed from independently drawn uniform pseudo-random numbers $\xi \in [0,1)$. New sample points are generated without regard to the previously generated sample points. Figure~\ref{fig:independent-points} shows an illustration of 64 and 256 independent random samples in a unit square.

One benefit of this approach is that the sampling routine is trivially \textit{progressive}: we can continually add new samples without needing to know how many total sample points we will be generating. It is also trivial to generate samples in arbitrary dimensions by simply drawing a canonical random number $\xi$ for each dimension of a sample point. Unfortunately, however, since samples are drawn completely independently, the resulting sampling patterns many contain many samples which fall in close proximity while large regions of the sampling domain receive no samples.

Independent random sampling leads to the most basic form of Monte Carlo integration. The variance of a Monte Carlo estimator using independent random samples has a convergence rate of $\mathcal{O}(N^{-1})$. This result can be easily derived from the definitions of expected value and variance, but it can likewise be derived in the frequency domain from the periodogram of the sampling pattern. An important characteristic of this convergence rate is that (in contrast to deterministic quadrature techniques) the dimensionality of the integration problem does not appear in the expression for the convergence rate. Monte Carlo integration using independent random samples therefore does not suffer from the so-called ``curve of dimensionality''. This is one of the reasons why Monte Carlo techniques are so attractive for high-dimensional integration problems like the rendering of global illumination. Unfortunately, this convergence rate also means that error will always go down relatively slowly: we need four times as many samples to reduce the error by a factor of two.

Due to these limitations, many more sophisticated sampling techniques have been developed over the years which relax the assumption of independence in the hopes of reducing variance or improving convergence rate.

\subsection{Stratified/Jittered sampling}
Stratified sampling divides the integration domain into a number of disjoint regions (a.k.a ``strata'' or ``cells''), and places one or more independent random samples within each region. Jittered sampling, introduced to graphics by Cook and colleagues~\cite{Cook:1986:SSC}, is a restricted form of stratified sampling where the strata are of equal size with each receiving exactly one sample. Jittered sampling of a 2D unit square involves dividing the square into a grid of $N = M\times M$ square cells, and placing a single sample randomly positioned inside each cell. For 2D, this process would result in $N^2$ samples. Figure~\ref{fig:jittered-points} shows an illustration of $8\times8 =64$ and $16\times16 = 256$ jittered sample points in a unit square.

Jittered sampling considerably reduces the likelihood that many samples will fall within close proximity, and, since each stratum is guaranteed one sample, also places a strict upper bound on the size of any region void of any samples. These two properties considerably improve how well evenly distributed the sample points are compared to independent random sampling. Unfortunately, this improvement comes at the cost of some inconvenience. Jittered sampling is not progressive, so we cannot continually add more samples, and we must know the total number of samples before constructing the point set. Furthermore, the total number of samples is only set indirectly through the number of subdivisions along each dimension. With the same number of subdivisions along each dimension, the total number of samples has to be a perfect square in 2D, a perfect cube in 3D, etc. This can be cumbersome, and becomes increasingly so as the dimensionality of the point set grows. Setting the total number of samples with fine granularity becomes increasing difficult, e.g. dividing each dimension of a five-dimensional domain into two cells results in $2^5 = 32 samples$ and increasing the divisions to three per dimension already increases the sample count to $3^5 = 243$. While it is possible to set an different number of subdivisions along each dimension to obtain finer granularity, it is not easy to determine these values automatically or optimally for a particular integration problem.

It can be shown~\cite{} that jittered sampling cannot increase the variance of a Monte Carlo estimator compared to using independent random samples. In fact, it can often reduce variance considerably. Moreover, analysis of the periodogram reveals that jittered sampling has can result in asymptotically better convergence rates of XXX and YYY.

\subsection{Uniform jittered sampling}
Uniform jittered sampling is a further restriction of jittered sampling where the random location of a point within its stratum is the same across the entire point set. In other words, a single jitter is applied to all samples of a regular uniform pattern (as opposed to jittering each stratum's sample independently as in jittered/stratified sampling). For the 2D unit square, any single realization of a uniform jittered sampling pattern is simply a randomly shifted rectangular grid pattern. Figure~\ref{fig:uniform-jittered-points} shows two independent realizations of 256 uniform jittered points.

Uniform jitter sampling can be beneficial for some low-dimensional problems in rendering. For instance, Pauly et al.~\cite{pauly00metropolis} used uniform jittered sampling for performing the one-dimensional line integral of radiance along each camera ray, and Ramamoorthi and colleagues~\cite{Ramamoorthi:2012} analyzed the error of uniform jittered sampling for integrating visibility from area light sources.


\subsection{Uncorrelated Jitter, N-Rooks, and Latin hypercube sampling}
To overcome the issue of generating jittered sample points in high dimensions, Cook et al.~\cite{Cook:1984:DRT} proposed to randomly combine jittered sample sets over subspaces of the full domain. For instance, to generate sample distributions for a nine dimensional space, we could separately partitioning four two-dimensional subspaces (e.g. pixel area, camera lens area, reflection direction, and shadow ray direction) as well as one one-dimensional subspace (time) and then combine the subspace dimensions at random. He called this technique uncorrelated jittering.

In 2D, generating $N$ samples would entail creating two separate 1D jittered point sets $X$ and $Y$ with $N$ samples each, and then using the samples in $X$ as the $x$-coordinate of each 2D point, and couple each $x$ with a $y$ from $Y$ in randomly permuted order. Another way to view this is that we must divide the 2D domain into $N$ rows and $N$ columns, and ensuring that example one 2D sample falls within any row or column. Shirley~\cite{Shirley91-DQMSD} called this idea $N$-Rooks sampling since the sample positions correspond to the arrangement of rooks on an $N\times N$ chessboard where none of the rooks can capture another in a single move. Figure~\ref{fig:jittered-points} shows an illustration of N-Rooks sampling patterns with $64$ and $256$ points in a unit square.

Combining $D$ randomly permuted 1D jittered samples in this way for arbitrary dimension $D$ is called Latin hypercube sampling. Like jittered sampling, this sampling approach is not progress as we need to know the total number of samples in advance. However, we can easily use an arbitrary number of samples, and no longer have to deal with the difficulty of specifying a $N = M^D$ samples for a $D$ dimensional domain.

Jittered sampling and Latin-hypercube sampling are related, but offer different stratification guarantees, and therefore have different variance reduction behavior. As an illustrative example, a 2D jittered sampling pattern with $N$ points will ensure that few samples can clump together in 2D, but as many as $\sqrt{N}$ samples may project to exactly the same location along either of the two axes (in other words, the projections actually correspond to $\sqrt{N}$ strata with $\sqrt{N}$ points in each stratum). On the other hand, a 2D Latin-hypercube sampling pattern with $N$ points provides less guarantees of clumping reduction in 2D, but ensures that along either of the axis projections each of the $N$ sample points falls into a different stratum. Latin-hypercube sampling can lead to better variance and convergence if the integrand does not depend strongly on some of the dimensions. In the extreme, where the integrand depends only on the $x$ coordinate, but not on $y$, Latin-hypercube sampling would provide better stratification of the relevant dimensions than jittered sampling.

\subsection{Multi-Jittered sampling}
Chiu et al.~\cite{chiu94multi} introduce multi-jittered sampling, which effectively enforces the stratification guarantees of jittered sampling and N-rooks sampling simultaneously: for $N=M\times M$ points in 2D we generate exactly one sample in each of the $M\times M$ strata, but also guarantee that there is exactly one sample in each of the $N$ rows and $N$ columns (projected strata along the coordinate axes). Figure~\ref{fig:jittered-points} shows an illustration of $8\times8 = 64$ and $16\times16 = 256$ multi-jittered sample points in a unit square.

\section{Quasi-Monte Carlo}
\subsection{Halton sequence}
\TBC
\subsection{Hammersley sequence}
\TBC

\input{blueNoise}

%
\input{Pictures/fig-points-powspec-radialmean}
%

\section{Synthesis of sampling patterns with targeted spectral profiles}

Recently, researchers have noticed the importance of having a \emph{spectral control} on sampling methods. 
The idea behind having a sampler with spectral control is to be able to generate samples %which are guided 
from the pre-assigned Fourier characteristics. Consequently, the samples can spatially arrange themselves according 
to the characteristics assigned in the spectral domain. Parker and colleagues~\cite{Parker:1991:PSHP} were the first to propose an algorithm for manipulating the
power spectra of blue noise halftone patterns. A blue noise spectrum
defined by a step function is used as input to influence the
sample distributions. Recently, Zhou and colleagues~\cite{Zhou:2012} construct point sets matching a Fourier power spectrum function by performing 
a gradient descent optimization on an energy derived from the autocorrelation function. 
Such a Fourier power spectrum function can be either obtained from a known sampling
method, or completely constructed by the user.


% %----------------------------------------------------------------------------------------
% %	CHAPTER 6
% %----------------------------------------------------------------------------------------
% \chapterimage{ch6.pdf} % Chapter heading image
% \chapter{Case studies}
% In this chapter, we will present a suite of tests that may be used to compare the efficacy of sampling patterns when used in numerical integration. We will then tabulate this comparison, identifying the strengths and weaknesses of the different sampling strategies described in chapter~\ref{ch:stateoftheart}. Our test suite will encompass a variety of integrands of various dimensionalities as well as smoothness criteria. We will make our code available to facilitate the comparison of any sampling patterns that may be proposed in the future with methods in our chosen set of algorithms. 
% 
% \section{1D integrands}
% \TBC 
% 
% \subsection{Smooth functions}
% \TBC 
% \subsection{Functions with discontinuities}
% \TBC 
% \section{2D integrands}
% \TBC 
% \subsection{Smooth functions}
% \TBC 
% \subsection{Functions with discontinuities}
% \TBC 
% \subsection{Sub-pixel integration}
% \TBC 
% \section{Rendering test suite}
% \TBC 

% 
% \section{Figure}\index{Figure}
% 
% \begin{figure}[h]
% \centering\includegraphics[scale=0.5]{placeholder}
% \caption{Figure caption}
% \end{figure}

% \chapter{Example slides}\label{ch:exslides}
% The remaining pages in this document contain representative slides for the content in chapter~\ref{ch:stateoftheart}.
% % % \cleardoublepage
% % \phantomsection
% % \setlength{\columnsep}{0.75cm}
% % \addcontentsline{toc}{chapter}{\textcolor{ocre}{Index}}
% % \printindex
% 
% %----------------------------------------------------------------------------------------
% 
% % \includepdf[pages={40-75}]{../SampleCourseSlides/example-wj.pdf}
% 
% 
%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------
\chapterimage{biblio.pdf} % Chapter heading image

% \chapter*{Bibliography}
% \nocite{*}
% \section*{Books}
% \addcontentsline{toc}{section}{Books}
% \printbibliography[heading=bibempty,type=book]
% \section*{Articles}
% \addcontentsline{toc}{section}{Articles}
% \printbibliography[heading=bibempty]
\bibliographystyle{unsrt}
\bibliography{bibliography.bib}
\addcontentsline{toc}{chapter}{\textcolor{ocre}{Bibliography}}

%----------------------------------------------------------------------------------------

% \includepdf[pages={40-69}]{../SampleCourseSlides/example-wj.pdf}


\end{document}